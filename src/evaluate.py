import pandas as pd
import joblib
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split
import mlflow
import yaml
import os


def load_params():
    params = {}
    try:
        with open("params.yaml", "r") as f:
            params = yaml.safe_load(f)
    except FileNotFoundError:
        print("ê²½ê³ : params.yaml íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        params = {
            "train": {"random_state": 42, "test_size": 0.2},
            "evaluate": {"metrics": ["mse", "r2_score", "mae", "rmse"]},
        }
    return params


if __name__ == "__main__":
    params = load_params()
    train_params = params.get("train", {})
    eval_params = params.get("evaluate", {})

    random_state = train_params.get("random_state", 42)
    test_size = train_params.get("test_size", 0.2)
    metrics = eval_params.get("metrics", ["mse", "r2_score", "mae", "rmse"])

    print("ğŸ“Š ëª¨ë¸ í‰ê°€ ì‹œì‘")
    print(f"   - í‰ê°€ ë©”íŠ¸ë¦­: {metrics}")

    with mlflow.start_run(nested=True, run_name="Model_Evaluation"):
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv("data/raw_data.csv")
        feature_columns = [col for col in df.columns if col.startswith("feature")]
        X = df[feature_columns]
        y_true = df["target"]

        # ë°ì´í„° ë¶„í•  (í›ˆë ¨ê³¼ ë™ì¼í•œ ë°©ì‹)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_true, test_size=test_size, random_state=random_state
        )

        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        scaler_path = os.path.join("model", "scaler.pkl")
        if not os.path.exists(scaler_path):
            print(f"ì˜¤ë¥˜: ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ {scaler_path}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            exit(1)

        scaler = joblib.load(scaler_path)
        X_test_scaled = scaler.transform(X_test)

        # ëª¨ë¸ ë¡œë“œ (ì—¬ëŸ¬ ëª¨ë¸ íƒ€ì… ì‹œë„)
        model_types = ["ridge", "lasso", "linear"]
        model = None
        model_type = None

        for mt in model_types:
            model_path = os.path.join("model", f"{mt}_model.pkl")
            if os.path.exists(model_path):
                model = joblib.load(model_path)
                model_type = mt
                print(f"âœ… {model_type} ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                break

        if model is None:
            print("ì˜¤ë¥˜: ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            exit(1)

        # ì˜ˆì¸¡ ìˆ˜í–‰
        y_pred = model.predict(X_test_scaled)

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        results = {}

        if "mse" in metrics:
            mse = mean_squared_error(y_test, y_pred)
            results["mse"] = mse
            mlflow.log_metric("mse", mse)
            print(f"âœ… MSE: {mse:.4f}")

        if "r2_score" in metrics:
            r2 = r2_score(y_test, y_pred)
            results["r2_score"] = r2
            mlflow.log_metric("r2_score", r2)
            print(f"âœ… RÂ² Score: {r2:.4f}")

        if "mae" in metrics:
            mae = mean_absolute_error(y_test, y_pred)
            results["mae"] = mae
            mlflow.log_metric("mae", mae)
            print(f"âœ… MAE: {mae:.4f}")

        if "rmse" in metrics:
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            results["rmse"] = rmse
            mlflow.log_metric("rmse", rmse)
            print(f"âœ… RMSE: {rmse:.4f}")

        # ì¶”ê°€ í†µê³„ ì •ë³´
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {len(y_test)}")
        print(f"ğŸ“Š ì˜ˆì¸¡ê°’ ë²”ìœ„: {y_pred.min():.2f} ~ {y_pred.max():.2f}")
        print(f"ğŸ“Š ì‹¤ì œê°’ ë²”ìœ„: {y_test.min():.2f} ~ {y_test.max():.2f}")

        # íŠ¹ì„± ì¤‘ìš”ë„ ì¶œë ¥
        if hasattr(model, "coef_"):
            feature_importance = dict(zip(feature_columns, model.coef_))
            print("ğŸ” íŠ¹ì„± ì¤‘ìš”ë„ (ê³„ìˆ˜):")
            for feature, coef in sorted(
                feature_importance.items(), key=lambda x: abs(x[1]), reverse=True
            ):
                print(f"   {feature}: {coef:.4f}")

        # MLflowì— íŒŒë¼ë¯¸í„° ë¡œê¹…
        mlflow.log_param("model_type", model_type)
        mlflow.log_param("test_size", test_size)
        mlflow.log_param("random_state", random_state)

        print("âœ… MLflow: ëª¨ë“  ë©”íŠ¸ë¦­ê³¼ íŒŒë¼ë¯¸í„°ê°€ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ê²°ê³¼ ìš”ì•½
        print("\nğŸ“‹ í‰ê°€ ê²°ê³¼ ìš”ì•½:")
        for metric, value in results.items():
            print(f"   {metric.upper()}: {value:.4f}")
